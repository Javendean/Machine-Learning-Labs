---
title: "Lab 5"
author: "Javendean Naipaul"
output: pdf_document
date: 
---


We will work with the diamonds dataset from last lecture:

```{r}
pacman::p_load(ggplot2) #this loads the diamonds data set too
?diamonds
diamonds$cut =      factor(diamonds$cut, ordered = FALSE)
diamonds$color =    factor(diamonds$color, ordered = FALSE)
#TO-DO
diamonds$clarity =  factor(diamonds$clarity, ordered = FALSE)
skimr::skim(diamonds)
```

Given the information above, what are the number of columns in the raw X matrix?

#10

Verify this using code:

```{r}
ncol(diamonds)
```

Would it make sense to use polynomial expansions for the variables cut, color and clarity? Why or why not?

#No. Cut, color, and clarity are categorical variables. So, even if you dummify them, and raise them to an arbitrary degree, they will remain unaffected.

Would it make sense to use log transformations for the variables cut, color and clarity? Why or why not?

#No. If you dummify them, there would only be two possible values for each dummy variable anyways, so a log transformation would just give two arbitrarily different values.

In order to ensure there is no time trend in the data, randomize the order of the diamond observations in D:.

```{r}
diamonds = diamonds[sample(1:nrow(diamonds)),]
```

Let's also concentrate only on diamonds with <= 2 carats to avoid the issue we saw with the maximum. So subset the dataset. Create a variable n equal to the number of remaining rows as this will be useful for later. Then plot it.

```{r}
diamonds = diamonds[diamonds$carat<=2,]
n = nrow(diamonds)
ggplot(diamonds, aes(x = carat, y = price)) + 
  geom_point()
```

Create a linear model of price ~ carat and gauge its performance using s_e.

```{r}
mod = lm(price~carat,diamonds)
summary(mod)$sigma
```

Create a model of price ~ clarity and gauge its performance

```{r}
modPrice = lm(price~clarity,diamonds)
summary(modPrice)$sigma
```

Why is the model price ~ carat substantially more accurate than price ~ clarity?

#Because clarity is a categorical variable and carat is nominal (discreet). Plus, carats simply affect price of a diamond more than clarity

Create a new transformed feature ln_carat and plot it vs price.

```{r}
diamonds$ln_carat = log(diamonds$carat)
ggplot(diamonds, aes(x = ln_carat, y = price)) + 
  geom_point()
```

Would price ~ ln_carat be a better fitting model than price ~ carat? Why or why not?

#price~ln_carat could be better than price~carat because there is "less" variance in price~ln_carat, but this could just be a matter of scale, so it can also be worse than price~carat. Plus, ln_carat lessens the distance between lower cut and larger cuts, so a model with ln_carat might under price higher cut diamonds and over price lesser cut diamonds, or vice versa.

Verify this by comparing R^2 and RMSE of the two models:

```{r}
mod1 = lm(price~carat,diamonds)
summary(mod1)$sigma
mod2 = lm(price~ln_carat,diamonds)
summary(mod2)$sigma
```

Create a new transformed feature ln_price and plot its estimated density:


```{r}
diamonds$ln_price = log(diamonds$price)
ggplot(diamonds) + geom_histogram(aes(x = ln_price), binwidth = 0.01)
```


Now plot it vs carat.

```{r}
ggplot(diamonds, aes(x = carat, y = ln_price)) + 
  geom_point()
```

Would ln_price ~ carat be a better fitting model than price ~ carat? Why or why not?

#Yes because the distribution of ln_price ~ carat is tighter than price ~ carat.

Verify this by computing s_e of this new model. Make sure these metrics can be compared apples-to-apples with the previous.

```{r}
mod3 = lm(ln_price~carat,diamonds)
yhat_price = exp(mod3$fitted.values)
SSE = sum((yhat_price - diamonds$price)^2)
sqrt(SSE/(n-2))
summary(mod1)$sigma
```

We just compared in-sample statistics to draw a conclusion on which model has better performance. But in-sample statistics can lie! Why is what we did valid?

#Because we're only using one feature, and have 53940 observations. 

Plot ln_price vs ln_carat.

```{r}
ggplot(diamonds, aes(x = ln_carat, y = ln_price)) + 
  geom_point()
```

Would ln_price ~ ln_carat be the best fitting model than the previous three we considered? Why or why not?

#Yes. The data looks the most linear here.

Verify this by computing s_e of this new model. Make sure these metrics can be compared apples-to-apples with the previous.

```{r}
mod4 = lm(ln_price~ln_carat,diamonds)
yhat_price = exp(mod4$fitted.values)
SSE = sum((yhat_price - diamonds$price)^2)
sqrt(SSE/(n-2))
summary(mod1)$sigma
```

Compute b, the OLS slope coefficients for this new model of ln_price ~ ln_carat.

```{r}
coef(mod4)
```

Interpret b_1, the estimated slope of ln_carat.

#The percent change in price is equal to the b1 times the percent change in carats.

Interpret b_0, the estimated intercept.

#The predicted price of a weightless diamond.

Create other features ln_x, ln_y, ln_z, ln_depth, ln_table.

```{r}
diamonds$ln_x = log(diamonds$x)
diamonds$ln_y = log(diamonds$y)
diamonds$ln_z = log(diamonds$z)
diamonds$ln_depth = log(diamonds$depth)
diamonds$ln_table = log(diamonds$table)
```

From now on, we will be modeling ln_price (not raw price) as the prediction target. 

Create a model (B) of ln_price on ln_carat interacted with clarity and compare its performance with the model (A) ln_price ~ ln_carat.

```{r}
ModA = lm(ln_price~ln_carat, diamonds)
ModB = lm(ln_price~ln_carat*clarity,diamonds)
summary(ModA)$sigma
summary(ModB)$sigma
```

Which model does better? Why?

#Model B does better because it gives a more dynamic pricing function that considers that diamonds with combinations of desirable carats and clarity are increasingly more valuable than dimaonds with undesirable and undesirable clarity.

Create a model of (C) ln_price on ln_carat interacted with every categorical feature (clarity, cut and color) and compare its performance with model (B)

```{r}
ModC = lm(ln_price ~ (ln_carat) * (clarity + cut  + color), diamonds)
summary(ModB)$sigma
summary(ModC)$sigma
```

Which model does better? Why?

#ModC does better because it gives a dynamic pricing function, with  a diffential slope, but it uses more features.  

Create a model (D) of ln_price on every continuous feature (logs of carat, x, y, z, depth, table) interacted with every categorical feature (clarity, cut and color) and compare its performance with model (C).

```{r}
ln_friendly_diamonds  = diamonds[which(diamonds$x>0 & diamonds$y>0 & diamonds$z>0),]
ModD = lm(ln_price ~ (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table) * (clarity + cut  + color), ln_friendly_diamonds)
summary(ModC)$sigma
summary(ModD)$sigma
```

Which model does better? Why?

#ModD does bette than ModC for the same reason that ModC does better than ModB. They both give a dynamic pricing function that use  diffential slopes but ModD uses more features, so it's slopes will be more "well-informed".

What is the p of this model D? Compute with code.

```{r}
ModD$rank
ncol(model.matrix(~ (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table) * (clarity + cut  + color), ln_friendly_diamonds))
```

Create model (E) which is the same as before except create include the raw features interacted with the categorical features and gauge the performance against (D).

```{r}
ModE = lm(ln_price ~ (carat + x + y + z + depth + table) * (clarity + cut  + color), ln_friendly_diamonds)
summary(ModD)$sigma
summary(ModE)$sigma
```

Which model does better? Why?

#ModE does better, but very slightly. It seems that this is because it is better to fit log price to raw characterisistcs, as mentioned before, in line 82. In this case, taking the log of the features seems to cause a reduction in the explanatory power of the features, because taking the log features "clump" more valuable diamonds closer together with less valuable diamonds.   

Create model (F) which is the same as before except also include also third degree polynomials of the continuous features interacted with the categorical features and gauge performance against (E). By this time you're getting good with R's formula syntax!

```{r}
ModF = lm(ln_price ~ (poly(carat,3) + poly(x,3) + poly(y,3) + poly(z,3) + poly(depth,3) + poly(table,3)) * (clarity + cut  + color), ln_friendly_diamonds)
summary(ModE)$sigma
summary(ModF)$sigma
```

Which model does better? Why?

#ModF does better because it is simply more complex than ModF.

Can you think of any other way to expand the candidate set curlyH? Discuss.

#You can use any linearly independent combinations of the features, like polynomials of higher degree, or wrapping them in trigonmetric functions.

We should probably assess oos performance now. Sample 2,000 diamonds and use these to create a training set of 1,800 random diamonds and a test set of 200 random diamonds. Define K and do this splitting:

```{r}
K = 10
set.seed(1984)
n_samp = 2000
splitIndex = nrow(D)*(1-1/K)
D = ln_friendly_diamonds[sample(1:(nrow(ln_friendly_diamonds)), n_samp), ]
Dtrain = D[1:splitIndex,]
Dtest = D[(splitIndex+1):nrow(D),]
```

Compute in and out of sample performance for models A-F. Use s_e as the metric (standard error of the residuals). Create a list with keys A, B, ..., F to store these metrics. Remember the performances here will be worse than before since before you're using nearly 52,000 diamonds to build a model and now it's only 1,800! 

```{r}
inSampleSE = list()
outSampleSE = list()
ModA = lm(ln_price ~ ln_carat, Dtrain)
inSampleSE[['A']] = sd(ModA$residuals)
inSampleSE[['B']] = sd(ModB$residuals)
inSampleSE[['C']] = sd(ModC$residuals)
inSampleSE[['D']] = sd(ModD$residuals)
inSampleSE[['E']] = sd(ModE$residuals)
inSampleSE[['F']] = sd(ModF$residuals)
outSampleSE[['A']] = sd(Dtest$ln_price - predict(ModA,Dtest))
outSampleSE[['B']] = sd(Dtest$ln_price - predict(ModB,Dtest))
outSampleSE[['C']] = sd(Dtest$ln_price - predict(ModC,Dtest))
outSampleSE[['D']] = sd(Dtest$ln_price - predict(ModD,Dtest))
outSampleSE[['E']] = sd(Dtest$ln_price - predict(ModE,Dtest))
outSampleSE[['F']] = sd(Dtest$ln_price - predict(ModF,Dtest))
sd(modPrice$residuals)
unlist(inSampleSE)
unlist(outSampleSE)
```

You computed oos metrics only on n_* = 200 diamonds. What problem(s) do you expect in these oos metrics?

#There not realiable because these oos metrics are generated from ten percent of the data. The problem with this makes the the oos metrics highly variable.

To do the K-fold cross validation we need to get the splits right and crossing is hard. I've developed code for this already. Run this code.

```{r}
set.seed(2000)
temp = rnorm(n)
folds_vec = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K+1)), include.lowest = TRUE, labels = FALSE)
head(folds_vec, 200)
```

Comment on what it does and how to use it to do a K-fold CV:

#It randomly picks k values such that the sampling of k values for the train-test split follows normal distribution. We can use this to do a K-fold by running the K-fold CV with these 200 k values to get more reliable oos metrics.


```{r}
oos_SE = array(NA, K) 
yHat = array(NA, nrow(ln_friendly_diamonds))
for (k in 1 : K){
  testIndex = which(folds_vec == k)
  trainIndex = setdiff(1 : (nrow(ln_friendly_diamonds)), testIndex)
  
  modf = lm(ln_price ~ (poly(carat,3) + poly(x,3) + poly(y,3) + poly(z,3) + poly(depth,3) + poly(table,3)) * (clarity + cut  + color), ln_friendly_diamonds[trainIndex,])
  yHat[testIndex] = predict(modf, ln_friendly_diamonds[testIndex,])
  
  oos_SE[k] = sd(ln_friendly_diamonds[testIndex,]$ln_price - yHat[testIndex]) 
}
oos_SE
sd(oos_SE)
mean(oos_SE)
```

Does K-fold CV help reduce variance in the oos s_e? Discuss.

#Yes it does because K fold gives consistent partitions, which gives consistent oos metrics.

Imagine using the entire rest of the dataset besides the 2,000 training observations divvied up into slices of 200. Measure the oos error for each slice on Model F in a vector s_e_s_F and compute the s_s_e_F and also plot it.

```{r}
#TO-DO
ggplot(data.frame(s_e_s_F = s_e_s_F)) + geom_histogram(aes(x = s_e_s_F))
```






